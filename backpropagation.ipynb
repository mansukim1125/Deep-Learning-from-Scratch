{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "backpropagation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNki5ChwA6FVd3Up4pSlG8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansukim1125/Deep-Learning-from-Scratch/blob/main/backpropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnY9z1UrRvkN"
      },
      "source": [
        "가중치 매개변수에 대한 손실 함수의 기울기를 구하는 방법은 수치 미분을 사용하였다. 하지만 이는 구현하기는 쉽지만 속도가 느리다는 단점이 있다. 이를 개선하기 위해 ***오차역전파법***이 등장하게 된다. 이는 가중치 매개변수의 기울기를 효과적으로 계산하는 방법이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn6zY-dVRzw9"
      },
      "source": [
        "# 계산 그래프\n",
        "계산 그래프는 계산 과정을 그래프로 나타낸 것이다. 다음의 예를 보자:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiEf1OMvGII-"
      },
      "source": [
        "문제 1: 현빈 군은 슈퍼에서 1개에 100원인 사과를 2개 샀습니다. 이때 지불 금액을 구하세요. 단, 소비세가 10% 부과됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iho3IHcTPH4c"
      },
      "source": [
        "# 역전파\n",
        "## 곱셈 노드의 역전파\n",
        "$z=xy$라는 식을 생각해 보자. 이 식의 미분은 다음과 같다.\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x}=y, \\frac{\\partial z}{\\partial y}=x$\n",
        "\n",
        "이처럼 곱셈 노드 역전파는 상류의 값($z$)에 순전파 때의 입력 신호를 서로 바꾸어 곱해 하류로 보낸다.\n",
        "\n",
        "## 덧셈 노드의 역전파\n",
        "$z=x+y$라는 식을 생각해 보자. 이 식의 미분은 다음과 같다.\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x}=1, \\frac{\\partial z}{\\partial y}=1$\n",
        "\n",
        "이처럼 덧셈 노드 역전파는 상류의 값에 1을 곱하고 하류로 보낸다. 따라서 상류의 값이 그대로 하류로 흐른다는 것을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMkmKNmuN3dj"
      },
      "source": [
        "# 단순한 계층 구현하기\n",
        "곱셈 계층(`MulLayer`)와 덧셈 계층(`AddLayer`)를 구현해 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t68uQtV6rS-E"
      },
      "source": [
        "## 곱셈 계층\n",
        "모든 계층은 `forward()`와 `backward()`라는 공통의 메서드를 갖도록 구현할 것이다. `forward()`는 순전파, `backward()`는 역전파를 의미한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RftmfGI3OpJD"
      },
      "source": [
        "class MulLayer:\n",
        "  def __init__(self):\n",
        "    self.x = None\n",
        "    self.y = None\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    \"\"\"순전파는 x, y를 곱해 반환합니다\"\"\"\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "    out = x * y\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    \"\"\"역전파는 상류의 값에 x, y를 바꾸어 곱한 후 반환합니다\"\"\"\n",
        "    dx = dout * self.y\n",
        "    dy = dout * self.x\n",
        "\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cAyICsRo06"
      },
      "source": [
        "그렇다면 161쪽의 그림 5-16 '사과 2개 구입' 예를 순전파와 역전파를 이용해 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsyNUX2IR5mH",
        "outputId": "c0aefb54-1a53-41c7-9068-80c6ea59e187"
      },
      "source": [
        "apple = 100\n",
        "apple_quantity = 2\n",
        "tax = 1.1\n",
        "\n",
        "apple_apple_quantity_layer = MulLayer()\n",
        "apple_apple_quantity_tax_layer = MulLayer()\n",
        "\n",
        "apple_price = apple_apple_quantity_layer.forward(apple, apple_quantity)\n",
        "total_price = apple_apple_quantity_tax_layer.forward(apple_price, tax)\n",
        "\n",
        "print(total_price)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "220.00000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtbGe0XFTfMb"
      },
      "source": [
        "또한 각 변수에 대한 미분은 `backward()`로 구할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSu-e6TQTj4x",
        "outputId": "a51dc9a3-1d79-43b1-fcd4-1f5cf8b8b6c7"
      },
      "source": [
        "dtotal_price = 1\n",
        "\n",
        "dapple_price, dtax = apple_apple_quantity_tax_layer.backward(dtotal_price)\n",
        "dapple, dapple_num = apple_apple_quantity_layer.backward(dapple_price)\n",
        "\n",
        "print(dapple, dapple_num, dtax)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2 110.00000000000001 200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1YxkXsOWQ3Y"
      },
      "source": [
        "## 덧셈 계층\n",
        "덧셈 계틍은 상류에서 내려온 미분을 그대로 하류로 보낸다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATSDK6pfWbRd"
      },
      "source": [
        "class AddLayer:\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def forward(self, x, y):\n",
        "    \"\"\"순전파는 입력된 x, y를 더해 반환합니다\"\"\"\n",
        "    out = x + y\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    \"\"\"역전파는 상류의 미분 값을 그대로 하류로 반환합니다\"\"\"\n",
        "    dx = dout * 1\n",
        "    dy = dout * 1\n",
        "    return dx, dy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuaRrUMtW45L"
      },
      "source": [
        "(163쪽 그림 5-17 구현하기)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olTf52k4rVE7"
      },
      "source": [
        "# 활성화 함수 계층 구현하기\n",
        "계산 그래프를 신경망에 적용해 보자. 먼저 활성화 함수인 ReLU와 Sigmoid 계층을 구현해 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saTAqOcYrhpi"
      },
      "source": [
        "## ReLU 계층\n",
        "수식은 다음과 같다:\n",
        "\n",
        "$\n",
        "y=\\left\\{\\begin{matrix}\n",
        "x & (x>0) \\\\\n",
        "0 & (x\\leq 0) \\\\\n",
        "\\end{matrix}\\right.\n",
        "$\n",
        "\n",
        "이다. 위 식의 $\\frac{\\partial y}{\\partial x}$은 아래와 같다:\n",
        "\n",
        "$\n",
        "\\frac{\\partial y}{\\partial x}=\\left\\{\\begin{matrix}\n",
        "1 & (x>0) \\\\\n",
        "0 & (x\\leq 0) \\\\\n",
        "\\end{matrix}\\right.\n",
        "$\n",
        "\n",
        "이와 같이 순전파 때의 입력($x$)가 0보다 크면 역전파는 상류의 값을 그대로 하류로 보낸다. 또 0 이하이면 0을 곱해 하류로 신호를 보내지 않는다. 이 ReLU 계층을 코드로 구현하면 다음과 같이 표현할 수 있다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwzpB2OAtHbJ"
      },
      "source": [
        "class ReLU:\n",
        "  def __init__(self):\n",
        "    self.mask = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    self.mask = (x <= 0) # 입력이 0이하인 원소는 True, 0보다 큰 원소는 False로 이루어진 np.array를 만듬\n",
        "    out = x.copy()\n",
        "    out[self.mask] = 0 # 입력이 0이하인 원소의 위치의 값은 0이 되어 신호를 보내지 않음\n",
        "\n",
        "    return out\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dout[self.mask] = 0 # 상류에서 넘어온 값(dout) 중 0이하인 원소의 위치의 값을 0으로 세트\n",
        "    dx = dout\n",
        "\n",
        "    return dx # 한 후 하류로 반환"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVy9RXEUK4dh"
      },
      "source": [
        "## Sigmoid 계층\n",
        "Sigmoid 함수는 다음의 식으로 표현된다:\n",
        "\n",
        "$y=\\frac{1}{1+e^{-x}}$\n",
        "\n",
        "이를 계산 그래프로 그리면 167쪽 그림 5-19로 표현할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2JzFMMWNc6W"
      },
      "source": [
        "### 1 단계\n",
        "'$/$'노드의 미분은 다음과 같이 구할 수 있다.\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x}=-\\frac{1}{x^2}$\n",
        "\n",
        "이는 다시 다음과 같이 표현할 수 있다;\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x}=-y^2$\n",
        "\n",
        "따라서 해당 노드의 역전파는 $-\\frac{\\partial L}{\\partial y}y^2$로 표현된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jf-w4jIPATy"
      },
      "source": [
        "### 2 단계\n",
        "'$+$'노드의 역전파는 상류의 값을 그대로 하류로 내보낸다. 따라서 해당 노드의 역전파는 그대로 $-\\frac{\\partial L}{\\partial y}y^2$ 이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybz2KRb1PUdl"
      },
      "source": [
        "### 3 단계\n",
        "'$exp$'노드는 $y=e^x$연산을 수행하며 그 미분은 다음과 같다:\n",
        "\n",
        "$\\frac{\\partial y}{\\partial x}=e^x$\n",
        "\n",
        "따라서 해당 노드의 역전파는 $-\\frac{\\partial L}{\\partial y}y^2e^{-x}$ 이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DBesFbSQE14"
      },
      "source": [
        "### 4 단계\n",
        "'$\\times $'노드는 순전파 때의 값을 서로 바꾸어 곱한다. 따라서 해당 노드의 역전파는 $\\frac{\\partial L}{\\partial y}y^2e^{-x}$ 이다. 이는 역전파의 최종 출력이 된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0Rr3TQkQpPA"
      },
      "source": [
        "또한 이를 169쪽 그림 5-21처럼 Sigmoid 노드 하나로 대체할 수 있다. 이는 역전파 과정의 중간 계산들을 생략할 수 있어 더 효율적인 계산이라 할 수 있다. 또한 $\\frac{\\partial L}{\\partial y}y^2e^{-x}$ 는 다음처럼 정리할 수 있다.\n",
        "\n",
        "$\\begin{align}\n",
        "\\frac{\\partial L}{\\partial y}y^{2}e^{-x}&=\\frac{\\partial L}{\\partial y}\\frac{1}{(1+e^{-x})^{2}}e^{-x}\\\\&=\\frac{\\partial L}{\\partial y}\\frac{1}{1+e^{-x}}\\frac{e^{-x}}{1+e^{-x}}\\\\&=\\frac{\\partial L}{\\partial y}y(1-y)\n",
        "\\end{align}$\n",
        "\n",
        "이처럼 Sigmoid 계층의 역전파는 순전파의 출력($y$)만으로 계산할 수 있다. 그렇다면 이를 코드로 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mFBkbkayIyU"
      },
      "source": [
        "class Sigmoid:\n",
        "  def __init__(self):\n",
        "    self.out = None\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = 1 / (1 + np.exp(-x)) # Sigmoid 함수를 구현하고 순전파의 출력(out)으로 저장\n",
        "    self.out = out\n",
        "\n",
        "    return out # 순전파 출력\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dx = dout * (1.0 - self.out) * self.out # 위 간소화된 식을 구현\n",
        "\n",
        "    return dx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWZtc3R9rfm-"
      },
      "source": [
        "# Affine/Softmax 계층 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjLB1PSZrk2q"
      },
      "source": [
        "## Affine 계층\n",
        "신경망의 순전파 때 수행하는 행렬의 곱을 기하학에서는 Affine Transformation이라고 한다. 이러한 변환을 수행하는 처리를 Affine 계층이라고 한다. 이를 식으로 표현하면 다음과 같다:\n",
        "\n",
        "$Y=X\\cdot W + B$\n",
        "\n",
        "이를 계산 그래표로 표현하면 172쪽 그림 5-24와 같이 표현할 수 있다. 이의 역전파를 구해 보자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvNUF8_It2u1"
      },
      "source": [
        "### 1 단계\n",
        "먼저 $Y$는 $\\frac{\\partial L}{\\partial Y}$로 표현할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YzixiDluUs1"
      },
      "source": [
        "### 2 단계\n",
        "덧셈 노드는 상류의 값을 그대로 하류로 보내므로 $X\\cdot W$와 $B$모두 $\\frac{\\partial L}{\\partial Y}$로 표현된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3RQyQ6DukyD"
      },
      "source": [
        "### 3 단계\n",
        "$dot$은 곱셈 노드와 성질이 같다. 따라서 입력을 서로 교차해 곱해야 한다.\n",
        "\n",
        "예를 들어, $Y$의 형상이 $1\\times 3$이고, $X$의 형상은 $1\\times 2$이며 $W$의 형상은 $2\\times 3$이라고 하자. 우리가 구하고자 하는 것은 $\\frac{\\partial L}{\\partial X}$이고, $X$와 형상이 같다.\n",
        "\n",
        "$\\begin{align}\n",
        "X&=(x_0, x_1, \\cdots, x_n)\\\\\\frac{\\partial L}{\\partial X}&=(\\frac{\\partial L}{\\partial x_0}, \\frac{\\partial L}{\\partial x_1}, \\cdots, \\frac{\\partial L}{\\partial x_n})\n",
        "\\end{align}$\n",
        "\n",
        "그렇다면 $\\frac{\\partial L}{\\partial Y}$와 $W$를 이용해 $\\frac{\\partial L}{\\partial X}$의 형상인 $1\\times 2$를 만들어내려면 어떻게 해야 할까? 다음의 식을 보자.\n",
        "\n",
        "$\\frac{\\partial L}{\\partial X}=\\frac{\\partial L}{\\partial Y}\\cdot W^{T}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial X}$도 이와 같이 구할 수 있다."
      ]
    }
  ]
}