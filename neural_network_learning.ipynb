{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "neural-network-learning",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMc5Wk/UHihlBZ23WbTO2h/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mansukim1125/Deep-Learning-from-Scratch/blob/main/neural_network_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtRiEZ6XLqo8"
      },
      "source": [
        "시작하기 전, 학습에 필요한 데이터 셋이나 예제 코드를 Github에서 clone해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4b7OTUbL3GK",
        "outputId": "33621447-cb14-4abe-d577-21a2aabecb49"
      },
      "source": [
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch'...\n",
            "remote: Enumerating objects: 826, done.\u001b[K\n",
            "remote: Total 826 (delta 0), reused 0 (delta 0), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (826/826), 52.21 MiB | 22.72 MiB/s, done.\n",
            "Resolving deltas: 100% (477/477), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RW-Mqi0Bzzf"
      },
      "source": [
        "# 학습이란?\n",
        "* 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것을 말함\n",
        "\n",
        "# 데이터\n",
        "## 훈련 데이터, 시험 데이터\n",
        "1. 기계 학습 문제는 데이터를 훈련 데이터와 시험 데이터로 나누고, (범용 능력을 평가하기 위해)\n",
        "2. 훈련 데이터만 사용해 학습하면서 최적의 매개변수를 찾는다.\n",
        "3. 시험 데이터를 사용해 앞서 훈련한 모델의 실력을 평가.\n",
        "따라서 여러 데이터 셋을 이용해 매개변수의 학습과 평가를 수행해야 한다. 그렇지 않고 하나의 데이터 셋을 이용하면 해당 데이터 셋에 *편향된* **오버피팅(overfitting. 과적합) 상태**가 될 수 있다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1f6_zCEsq2j"
      },
      "source": [
        "# 손실 함수\n",
        "* 신경망이 학습할 수 있도록 해주는 지표. 이 손실 함수의 값을 가장 작게 만드는 가중치 매개변수를 찾는 것이 '학습'의 목표\n",
        "\n",
        "신경망 학습에서는 현재의 상태를 '*하나의 지표*'로 표현한다. 그리고 그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것을 학습이라 한다. 신경망 학습에서 사용하는 *지표*를 **손실 함수**라 한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyvQBnMFswTm"
      },
      "source": [
        "## 평균 제곱 오차\n",
        "가장 많이 쓰이는 손실 함수로, 다음의 식으로 표현된다:\n",
        "\n",
        "$E= \\frac{1}{2}\\sum_{k}^{}(y_{k}-t_{k})^{2}$\n",
        "&nbsp;($y_{k}$는 신경망의 출력, $t_{k}$는 정답 레이블, $k$는 데이터의 차원 수)\n",
        "\n",
        "다음의 예를 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9Jq7tMQCVJa"
      },
      "source": [
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # one-hot encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmc8DaHqQFuq"
      },
      "source": [
        "위 배열 중 y은 MNIST의 손글씨 인식 예제의 출력이고, t는 정답 레이블이다. 여기서 정답은 t[2]가 1 이므로 2 이다. 그렇다면 평균 제곱 오차를 코드로 구현해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKNIdDhRQeQR"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "  return 0.5 * np.sum((y - t) ** 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayqUjl5SQy-R"
      },
      "source": [
        "그리고 이 함수에 위 y, t를 인자로 전달하고 실행해보자. 다음은 첫 번째 예이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOA3i7zzQ7K0",
        "outputId": "bd589c97-c5cf-48b9-8489-db61deef1a4d"
      },
      "source": [
        "mean_squared_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09750000000000003"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qMcQIikRQgR"
      },
      "source": [
        "이는 위 y,t 를 그대로 사용했다. 두 번째 예는 오차가 크게 나오도록 y(출력)중 정답에 해당하는 y[2]를 0.1로, y[7]을 0.6로 조정해 보았다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGYXS2nbRhqC",
        "outputId": "257c2a47-312a-4b5d-9ab3-8caea037e562"
      },
      "source": [
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "\n",
        "mean_squared_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5975"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0jYUyhISJIx"
      },
      "source": [
        "두 번째 결과를 보면 추정이 실패할 경우 정답과 오차가 크게 발생함을 알 수 있다. 또한 오차가 적은 첫 번째 추정 결과가 정답에 더 가깝다고 할 수 있다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH1kLSfrSx2h"
      },
      "source": [
        "## 교차 엔트로피 오차\n",
        "다음의 식으로 표현된다:\n",
        "\n",
        "$E= -\\sum_{k}^{}t_{k}\\log_{e}y_{k}$&nbsp;($y_{k}$는 신경망의 출력, $t_{k}$는 정답 레이블, $k$는 데이터의 차원 수)\n",
        "\n",
        "실질적으로 $y_{k}$중 $t_{k}$가 1인 원소에 해당하는 값만 $log_{e}y_{k}$를 구한다고 할 수 있다. (질문: 왜 sigma를 사용했는가?)\n",
        "\n",
        "다음은 $log_{e}x$의 그래프이다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US9TifWfWXIJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "31c3364c-8c8d-4b7a-ffc2-a5dc38159e75"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = np.arange(0.001, 1.0, 0.001)\n",
        "y = np.log(x)\n",
        "\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.plot(x, y)\n",
        "\n",
        "plt.ylim(-5, 0)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzCH0ZfPXzia"
      },
      "source": [
        "먼저 교차 엔트로피 오차를 코드로 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSUjhJhacxih"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  delta = 1e-7 # np.log(0)인 경우, -inf가 되어 계산할 수 없기 때문.\n",
        "  return -np.sum(t * np.log(y + delta))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3fvy615c5Pp"
      },
      "source": [
        "첫 번째 예는 정답이 2인 예이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR5lPyAEc0jR"
      },
      "source": [
        "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
        "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # one-hot encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vz0_YrCdNxJ",
        "outputId": "33a528ff-8c5e-40a1-8eca-b3d9fc4b3bb4"
      },
      "source": [
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.510825457099338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqswNRWHdqr6"
      },
      "source": [
        "두 번째 예는 위 평균 제곱 오차와 같은 예로, 오차가 크게 나오게끔 출력을 조정했다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfK9nS7RemjU",
        "outputId": "552bc305-d9e0-4a2a-f66e-6a07329af9ec"
      },
      "source": [
        "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
        "\n",
        "cross_entropy_error(np.array(y), np.array(t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.302584092994546"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc2HZ1RTewsS"
      },
      "source": [
        "위 예를 보면 정답일 때의 출력이 0.6으로, 교차 엔트로피 오차는 0.51이고, 두 번째 예를 보면 정답일 때의 출력이 0.1으로, 교차 엔트로피 오차는 2.30이다. 따라서 오차가 더 작은 추정(첫 번째 예)이 더 정답에 가깝다고 할 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huA7jxtoENqm"
      },
      "source": [
        "## 미니배치 학습\n",
        "기계학습 문제는 훈련 데이터에 대한 손실 함수의 값을 구하고, 그 값을 최대한 줄여주는 매개 변수를 찾아내는 것이다. 이렇게 하려면 먼저 모든 훈련 데이터를 대상으로 손실 함수 값을 구해야 한다. 즉 훈련 데이터가 100개 있으면 100개의 손실 함수의 값을 계산해 이의 합을 *지표*로 삼는 것이다.\n",
        "\n",
        "교차 엔트로피 오차의 식은 다음과 같이 변형된다:\n",
        "\n",
        "$E= -\\frac{1}{N}\\sum_{n}\\sum_{k}^{}t_{nk}\\log_{e}y_{nk}$&nbsp;($y_{nk}$는 신경망의 출력, $t_{nk}$는 정답 레이블, $k$는 데이터의 차원 수) (질문. 평균($\\frac{1}{N}\\sum_{n}$)을 내는 이유?)\n",
        "\n",
        "이 식은 위 교차 엔트로피 오차의 식을 $n$개의 데이터로 확장했을 뿐이다. 또한 마지막에 $N$으로 나누어 줌으로써 ***평균 손실 함수***를 구하는 것이다.\n",
        "\n",
        "그러나 여기서 모든 데이터에 대해 손실 함수의 합을 구하려면 시간이 오래 걸릴 것이다. 따라서 일부 데이터만 추출해서 전체의 *근사치*로 이용할 수 있다.\n",
        "\n",
        "그렇다면 MNIST 데이터 셋을 이용해 미니배치 학습을 한 번 구현해 보자. 먼저 Git Repo 디렉토리로 들어가 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQS5IKFZKi3L",
        "outputId": "ce9bf2f1-812c-4a51-a4d0-7cfa3bce787f"
      },
      "source": [
        "%cd deep-learning-from-scratch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deep-learning-from-scratch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjFYwP4Mcf-"
      },
      "source": [
        "다음은 MNIST 데이터 셋을 불러오는 코드이다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb1BBqbxNZBu",
        "outputId": "0cf32466-b274-4bf2-fef9-fd2773c89b65"
      },
      "source": [
        "from dataset.mnist import load_mnist\n",
        "\n",
        "\n",
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(t_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n",
            "(60000, 784)\n",
            "(60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ2MA66BN_ah"
      },
      "source": [
        "x_train은 훈련 데이터로, 60000개의 이미지 데이터, 각 데이터 당 784(28 * 28)픽셀로 구성되어 있다. 또 t_train은 정답 레이블로, 60000개의 이미지 데이터에 대한 숫자 레이블이다. 이 훈련 데이터에서 무작위로 10장만 추출해 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpLEiY-RO8wO",
        "outputId": "2ec5a880-0159-4216-cd65-78664a46bca1"
      },
      "source": [
        "train_size = x_train.shape[0] # 60000\n",
        "batch_size = 10\n",
        "batch_mask = np.random.choice(train_size, batch_size)\n",
        "\n",
        "print(batch_mask)\n",
        "\n",
        "x_batch = x_train[batch_mask] # 10 개의 훈련 데이터\n",
        "t_batch = t_train[batch_mask] # 10 개의 정답 레이블"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8072 28479 45588 21539 51777 50428  5561 38244  2713 31972]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7kqJuspXiDT"
      },
      "source": [
        "그렇다면, 위의 데이터의 교차 엔트로피 값을 구하기 위한 손실 함수를 코드로 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNABFkaBXqoc"
      },
      "source": [
        "def cross_entropy_error(y, t):\n",
        "  if y.ndim == 1:\n",
        "    t = t.reshape(1, t.size)\n",
        "    y = y.reshape(1, y.size)\n",
        "\n",
        "  batch_size = y.shape[0]\n",
        "  return -np.sum(t * np.log(y)) / batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SlQsDvBNtLxO"
      },
      "source": [
        "# 수치 미분\n",
        "경사법에서는 기울기(경사) 값을 기준으로 나아갈 방향을 결정한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QClFMzg0tTYb"
      },
      "source": [
        "## 미분\n",
        "미분(영어: derivative, 微分)은 어떤 함수의 정의역 속 각 점에서 함숫값의 변화량과 독립 변숫값의 변화량 비의 극한이다. 이는 다음의 식으로 표현될 수 있다:\n",
        "\n",
        "$\\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{f(x+h)-f(x)}{h}$\n",
        "\n",
        "이를 코드로 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaOKuJxOucOb"
      },
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-50 # 아주 작은 값\n",
        "  return (f(x + h) - f(x)) / h"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83gKAY3oux8N"
      },
      "source": [
        "위 코드는 $h$값으로 아주 작은 $10^{-50}$를 사용했다. 하지만 이를 실제 계산에 적용하면 다음과 같은 문제가 발생할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Unf-gs9av_zg",
        "outputId": "67bd1122-93e8-4b51-ca03-481e4d2919e5"
      },
      "source": [
        "np.float32(1e-50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "walgeK81y9z2"
      },
      "source": [
        "이와 같이 너무 작은 값을 이용해 계산하면 컴퓨터로 계산하는 데 문제가 발생할 수 있다. 그렇다면 그 정도로 작은 값은 아니지만 어느 정도로 작은 값인 $10^{-4}$를 $h$로 이용해 보자. 하지만 이렇게 하면 $f(x + h) - f(x)$간 차이가 커질 수밖에 없고 실제 기울기와 차이가 생길 수밖에 없다.\n",
        "\n",
        "따라서 이 오차를 줄이기 위해 $x + h$와 $x - h$일 때의 값의 중앙을 찾는 방법을 사용한다. 그렇다면 이러한 개선점들을 적용해 다시 코드를 작성해보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25NTxqY8XGdD"
      },
      "source": [
        "def numerical_diff(f, x):\n",
        "  h = 1e-4\n",
        "  return (f(x + h) - f(x - h)) / (2 * h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EwfWm4PXldz"
      },
      "source": [
        "위 코드는 다음의 식을 구현한 것이다:\n",
        "\n",
        "$\\frac{df(x)}{dx}=\\lim_{h \\to 0}\\frac{1}{2}\\frac{f(x+h)-f(x-h)}{h}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_S9EPuiYZVt"
      },
      "source": [
        "## 편미분\n",
        "다음의 함수를 보자:\n",
        "\n",
        "$f(x_{0}, x_{1})=x_{0}^{2}+x_{1}^{2}$\n",
        "\n",
        "이 함수에는 일반적인 함수와 다르게 매개변수가 2개이다. ($x_{1}, x_{2}$)\n",
        "\n",
        "이 함수를 코드로 구현해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2UblQqjZLSB"
      },
      "source": [
        "f = lambda x: x[0] ** 2 + x[1] ** 2 # x는 numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEMo1RvSblOn"
      },
      "source": [
        "그렇다면 위 함수를 미분해보자. 그렇다면 먼저 생각해야 할 것은 어느 변수에 대한 미분인가이다. 이와 같이 변수가 여럿인 함수에 대한 미분을 *편미분*이라 한다. 위 함수의 편미분을 수식으로는 $\\frac{\\partial f}{\\partial x_{0}}, \\frac{\\partial f}{\\partial x_{1}}$로 쓴다. 다음은 해당 함수의 편미분의 예이다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGfmNe33d75J"
      },
      "source": [
        "문제 1: $x_{0} = 3$, $x_{1} = 4$일 때, $x_{0}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{0}}$를 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfIm1irLcwqy",
        "outputId": "a8e9cc7b-c3da-4c34-dab1-615109615513"
      },
      "source": [
        "def f_x0(x0):\n",
        "  return x0 * x0 + 4.0 ** 2.0 # x1이 4이므로 미리 설정\n",
        "\n",
        "numerical_diff(f_x0, 3.0) # x0 에 대한 편미분"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.00000000000378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuMfNCleeAD8"
      },
      "source": [
        "문제 2: $x_{0} = 3$, $x_{1} = 4$일 때, $x_{1}$에 대한 편미분 $\\frac{\\partial f}{\\partial x_{1}}$를 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONI7Udz6eFGE",
        "outputId": "0fec08c8-952b-4474-e312-eff9392b22f1"
      },
      "source": [
        "def f_x1(x1):\n",
        "  return 3.0 ** 2 + x1 * x1\n",
        "\n",
        "numerical_diff(f_x1, 4.0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.999999999999119"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y5dz0ooCeTFM"
      },
      "source": [
        "이처럼 편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다. 단, 여러 변수 중 목표 변수 하나를 두고 다른 변수는 값을 고정해 그동안 사용했던 수치 미분 함수를 적용해 편미분을 구한 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TLyUqnie6PR"
      },
      "source": [
        "# 기울기\n",
        "앞에서는 $x_{0}$와 $x_{1}$의 편미분을 따로 계산했다. 하지만 이 둘의 편미분을 *동시에* 계산하고자 한다. 예를 들어 $x_{0} = 3$, $x_{1} = 4$일 때 이 둘의 편미분을 묶어 $(\\frac{\\partial f}{\\partial x_{0}}, \\frac{\\partial f}{\\partial x_{1}})$을 계산한다고 해보자. 이렇게 모든 변수의 편미분을 벡터로 정리한 것을 ***기울기***라고 한다. 다음의 코드는 기울기를 계산하는 코드이다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRSAbFWLmT5N"
      },
      "source": [
        "def numerical_gradient(f, x):\n",
        "  h = 1e-4\n",
        "  grad = np.zeros_like(x) # x와 같은 shape의 기울기 벡터\n",
        "\n",
        "  for idx in range(x.size):\n",
        "    # x는 numpy array이므로 특정 값(x[idx])을 temp로 가지고 있다가\n",
        "    # f(x)계산이 완료되면 다시 복원하는 것이 깔끔\n",
        "    temp = x[idx]\n",
        "    \n",
        "    x[idx] = temp + h # x + h\n",
        "    fx_plus_h = f(x)\n",
        "\n",
        "    x[idx] = temp - h # x - h\n",
        "    fx_minus_h = f(x)\n",
        "\n",
        "    grad[idx] = (fx_plus_h - fx_minus_h) / (2 * h) # 중심 차분\n",
        "    x[idx] = temp\n",
        "\n",
        "  return grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ClhPmzknrQ8"
      },
      "source": [
        "그렇다면 이 코드를 이용해 실제로 기울기를 구해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UNOL7cHZnyYq",
        "outputId": "8de79fff-871b-4a95-8ba0-b908e5daf38c"
      },
      "source": [
        "numerical_gradient(f, np.array([3.0, 4.0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([6., 8.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzANa0eLoMbU"
      },
      "source": [
        "위에 계산한 결과와 거의 일치함을 알 수 있다. 또한 손실 함수의 값이 가장 작은 지점에서의 가중치 매개변수를 찾는 것이 바로 다음에 언급할 경사법의 목표라 할 수 있다. 그때 사용할 수 있는 지표가 바로 기울기이다. 하지만 기울기가 0이라고 그 값이 반드시 해당 함수의 최솟값이라고 할 수는 없다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTu-07m1Kcl9"
      },
      "source": [
        "## 경사 하강법\n",
        "위에서 설명했듯, 기울어진 방향이 꼭 최솟값을 가리키는 것은 아니나, 최소한 그 방향으로 가야 함수의 값을 줄일 수 있다. 그래서 경사 하강법은 기울기를 이용해 나아갈 방향을 정해야 한다. 경사법은 기계 학습을 ***최적화***하는 데 흔히 쓰는 방법이다. 아래는 경사법을 수식으로 표현한 것이다:\n",
        "\n",
        "$x_{0}=x_{0}-\\eta \\frac{\\partial f}{\\partial x_{0}}$\n",
        "\n",
        "$x_{1}=x_{1}-\\eta \\frac{\\partial f}{\\partial x_{1}}$\n",
        "\n",
        "($\\eta$는 학습률)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyCAVhaHPqV_"
      },
      "source": [
        "$\\eta$는 신경망 학습에서는 학습률로, 한 번의 학습으로 매개변수 값을 얼마나 갱신하는지를 정하는 인자이다. 위 식은 1회에 해당하는 갱신이고, 이를 반복해 서서히 함수의 값을 줄이는 것이다. 이 값은 미리 특정 값으로 정해두어야 하는데, 일반적으로 이 값이 너무 크거나 작으면 *좋은 장소*를 찾기 어렵다. 다음은 경사 하강법을 코드로 구현한 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E2N7tl3QcZi"
      },
      "source": [
        "def gradient_descent(f, x, lr=0.01, step_num=100, verbose=False):\n",
        "  x_history = []\n",
        "  x_history.append(x)\n",
        "  for i in range(step_num):\n",
        "    grad = numerical_gradient(f, x)\n",
        "    x -= lr * grad\n",
        "    print(x)\n",
        "    x_history.append(x)\n",
        "\n",
        "  if verbose is True: return (x, x_history, )\n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGuMSWX8Vnot"
      },
      "source": [
        "f는 최적화하려는 함수, x는 초깃값, lr은 learning rate, step_num은 경사법에 따른 반복 횟수이다. 함수의 기울기는 numerical_gradient로 구하고, 이를 stem_num만큼 반복해 x를 갱신한다. 다음은 경사 하강법의 예이다:\n",
        "\n",
        "문제: 경사법으로 $f(x_{0}, x_{1})=x_{0}^{2}+x_{1}^{2}$의 최솟값을 구하라."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6bbdPSxXkqz",
        "outputId": "782f8f5d-661a-463c-8b7e-590479a1b052"
      },
      "source": [
        "x = np.array([-3.0, 4.0])\n",
        "gradient_descent(f, x, lr=0.1, step_num=100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-6.11110793e-10,  8.14814391e-10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLWOocLGdbHQ"
      },
      "source": [
        "아래는 이 과정을 표현한 그래프이다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yULX_x-4dfLC",
        "outputId": "bde1b327-ed8d-4a7e-b276-4acbfe8308b5"
      },
      "source": [
        "x = np.array([-3.0, 4.0])\n",
        "x, x_history = gradient_descent(f, x, lr=0.1, step_num=100, verbose=True)\n",
        "\n",
        "print(x, x_history)\n",
        "\n",
        "# plt.plot"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-2.4  3.2]\n",
            "[-1.92  2.56]\n",
            "[-1.536  2.048]\n",
            "[-1.2288  1.6384]\n",
            "[-0.98304  1.31072]\n",
            "[-0.786432  1.048576]\n",
            "[-0.6291456  0.8388608]\n",
            "[-0.50331648  0.67108864]\n",
            "[-0.40265318  0.53687091]\n",
            "[-0.32212255  0.42949673]\n",
            "[-0.25769804  0.34359738]\n",
            "[-0.20615843  0.27487791]\n",
            "[-0.16492674  0.21990233]\n",
            "[-0.1319414   0.17592186]\n",
            "[-0.10555312  0.14073749]\n",
            "[-0.08444249  0.11258999]\n",
            "[-0.06755399  0.09007199]\n",
            "[-0.0540432   0.07205759]\n",
            "[-0.04323456  0.05764608]\n",
            "[-0.03458765  0.04611686]\n",
            "[-0.02767012  0.03689349]\n",
            "[-0.02213609  0.02951479]\n",
            "[-0.01770887  0.02361183]\n",
            "[-0.0141671   0.01888947]\n",
            "[-0.01133368  0.01511157]\n",
            "[-0.00906694  0.01208926]\n",
            "[-0.00725355  0.00967141]\n",
            "[-0.00580284  0.00773713]\n",
            "[-0.00464228  0.0061897 ]\n",
            "[-0.00371382  0.00495176]\n",
            "[-0.00297106  0.00396141]\n",
            "[-0.00237684  0.00316913]\n",
            "[-0.00190148  0.0025353 ]\n",
            "[-0.00152118  0.00202824]\n",
            "[-0.00121694  0.00162259]\n",
            "[-0.00097356  0.00129807]\n",
            "[-0.00077884  0.00103846]\n",
            "[-0.00062308  0.00083077]\n",
            "[-0.00049846  0.00066461]\n",
            "[-0.00039877  0.00053169]\n",
            "[-0.00031901  0.00042535]\n",
            "[-0.00025521  0.00034028]\n",
            "[-0.00020417  0.00027223]\n",
            "[-0.00016334  0.00021778]\n",
            "[-0.00013067  0.00017422]\n",
            "[-0.00010453  0.00013938]\n",
            "[-8.36277945e-05  1.11503726e-04]\n",
            "[-6.69022356e-05  8.92029808e-05]\n",
            "[-5.35217885e-05  7.13623846e-05]\n",
            "[-4.28174308e-05  5.70899077e-05]\n",
            "[-3.42539446e-05  4.56719262e-05]\n",
            "[-2.74031557e-05  3.65375409e-05]\n",
            "[-2.19225246e-05  2.92300327e-05]\n",
            "[-1.75380196e-05  2.33840262e-05]\n",
            "[-1.40304157e-05  1.87072210e-05]\n",
            "[-1.12243326e-05  1.49657768e-05]\n",
            "[-8.97946606e-06  1.19726214e-05]\n",
            "[-7.18357285e-06  9.57809713e-06]\n",
            "[-5.74685828e-06  7.66247770e-06]\n",
            "[-4.59748662e-06  6.12998216e-06]\n",
            "[-3.67798930e-06  4.90398573e-06]\n",
            "[-2.94239144e-06  3.92318858e-06]\n",
            "[-2.35391315e-06  3.13855087e-06]\n",
            "[-1.88313052e-06  2.51084069e-06]\n",
            "[-1.50650442e-06  2.00867256e-06]\n",
            "[-1.20520353e-06  1.60693804e-06]\n",
            "[-9.64162827e-07  1.28555044e-06]\n",
            "[-7.71330261e-07  1.02844035e-06]\n",
            "[-6.17064209e-07  8.22752279e-07]\n",
            "[-4.93651367e-07  6.58201823e-07]\n",
            "[-3.94921094e-07  5.26561458e-07]\n",
            "[-3.15936875e-07  4.21249167e-07]\n",
            "[-2.52749500e-07  3.36999333e-07]\n",
            "[-2.02199600e-07  2.69599467e-07]\n",
            "[-1.61759680e-07  2.15679573e-07]\n",
            "[-1.29407744e-07  1.72543659e-07]\n",
            "[-1.03526195e-07  1.38034927e-07]\n",
            "[-8.28209562e-08  1.10427942e-07]\n",
            "[-6.62567649e-08  8.83423532e-08]\n",
            "[-5.30054119e-08  7.06738826e-08]\n",
            "[-4.24043296e-08  5.65391061e-08]\n",
            "[-3.39234636e-08  4.52312849e-08]\n",
            "[-2.71387709e-08  3.61850279e-08]\n",
            "[-2.17110167e-08  2.89480223e-08]\n",
            "[-1.73688134e-08  2.31584178e-08]\n",
            "[-1.38950507e-08  1.85267343e-08]\n",
            "[-1.11160406e-08  1.48213874e-08]\n",
            "[-8.89283245e-09  1.18571099e-08]\n",
            "[-7.11426596e-09  9.48568795e-09]\n",
            "[-5.69141277e-09  7.58855036e-09]\n",
            "[-4.55313022e-09  6.07084029e-09]\n",
            "[-3.64250417e-09  4.85667223e-09]\n",
            "[-2.91400334e-09  3.88533778e-09]\n",
            "[-2.33120267e-09  3.10827023e-09]\n",
            "[-1.86496214e-09  2.48661618e-09]\n",
            "[-1.49196971e-09  1.98929295e-09]\n",
            "[-1.19357577e-09  1.59143436e-09]\n",
            "[-9.54860614e-10  1.27314749e-09]\n",
            "[-7.63888491e-10  1.01851799e-09]\n",
            "[-6.11110793e-10  8.14814391e-10]\n",
            "[-6.11110793e-10  8.14814391e-10] [array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10]), array([-6.11110793e-10,  8.14814391e-10])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx7dIgkpcjMS"
      },
      "source": [
        "이 예는 초깃값을 $(-3.0, 4.0)$으로 설정하고 경사법을 이용해 최솟값 탐색을 수행한다. 최종 결과는 $(-6.11110793e-10, 8.14814391e-10)$으로 거의 $(0, 0)$에 수렴한다.\n",
        "\n",
        "(학습률이 너무 크거나 작은 경우 실험)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjggkJ4GjrJE"
      },
      "source": [
        "## 신경망에서의 기울기\n",
        "신경망의 학습에서는 가중치 매개변수에 대한 손실 함수의 기울기를 구해야 한다. 예를 들어 가중치가 $W$, 손실 함수가 $L$인 신경망을 생각해 보자:\n",
        "\n",
        "$W=\\begin{pmatrix}\n",
        "w_{11} & w_{21} & w_{31}\\\\ \n",
        "w_{12} & w_{22} & w_{32}\n",
        "\\end{pmatrix}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W}=\\begin{pmatrix}\n",
        "\\frac{\\partial L}{\\partial W_{11}} & \\frac{\\partial L}{\\partial W_{21}} & \\frac{\\partial L}{\\partial W_{31}}\\\\ \n",
        "\\frac{\\partial L}{\\partial W_{12}} & \\frac{\\partial L}{\\partial W_{22}} & \\frac{\\partial L}{\\partial W_{32}}\n",
        "\\end{pmatrix}$\n",
        "\n",
        "$\\frac{\\partial L}{\\partial W}$의 각 원소는 각각의 원소에 대한 편미분이다. 예를 들어 $\\frac{\\partial L}{\\partial W_{11}}$은 $w_{11}$을 조금 변경했을 때 $L$이 얼마나 변화하는지를 나타낸다. 여기서 중요한 것은 $W$와 $\\frac{\\partial L}{\\partial W}$의 형상이 같다는 것이다. 그렇다면 간단한 신경망에서 실제로 기울기를 구해 보자:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdL93aH2ozBn"
      },
      "source": [
        "class simpleNet:\n",
        "  def __init__(self):\n",
        "    self.W = np.random.randn(2, 3)\n",
        "\n",
        "  def predict(self, x):\n",
        "    return np.dot(x, self.W)\n",
        "\n",
        "  def loss(self, x, t):\n",
        "    z = self.predict(x)\n",
        "    y = softmax(z)\n",
        "    loss = cross_entropy_error(y, t)\n",
        "\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}